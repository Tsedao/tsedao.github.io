[{"content":"Who I am? My name is Zitao Song (Pronunced by Tsedao-Soong).\nI\u0026rsquo;m an MSc student studying in the Data Science Programme at The Chinese University of Hong Kong, Shenzhen.\nI\u0026rsquo;m primarily interested in modifying and designing modern machine learning algorithms to solve real-life problems.\nI like digging deep into the math behind the algorithms and trying to make connections between different algorithms.\nPersonally, I consider the EM-style algorithms the most elegant algorithms, since it generates a monotonic bounded sequence, which must converge (Bolzano-Weierstrass Theorem).\nMy current focus areas are hierarchy and goal conditioned reinforcement learning algorithms on portfolio management.\nPublications Song Z, Huang D, Song B, et al. Attention-based multi-label neural networks for integrated prediction and interpretation of twelve widely occurring RNA modifications[J]. Nature communications, 2021, 12(1): 1-11.\n","description":"About","image":null,"permalink":"https://tsedao.github.io/about/","subtitle":null,"tags":null,"title":"About"},{"content":"Heap memory and Stack memory   Different ways to initialize pointers\n int * a = \u0026amp;num; create a pointer points to num on stack memory Cube * cube = new Cube; create a pointer points to cube on heap memory    When deleting a pointer, we need to assign the pointer to nullptr even if the the memory will be released after the function is returned\n  Pointer can use -\u0026gt; to obtain the class attribute, otherwise we need dereference and use dot (cube-\u0026gt;attribute \u0026lt;=\u0026gt; (*cube).attribute)\n  1 2  int *x = new int; int \u0026amp;y = *x; //y is the alias of heap memory stored *x     Constructors   Default constructor/ Customized constructor\n  Copy constructor (copy constructors are invoked automatically)\n  Passing an object as a parameter (by value)\n  Returning an object from a function (by value)\n  Initializing a new object\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  Cube::Cube(const Cube \u0026amp; obj) { length_ = obj.length_; std::cout \u0026lt;\u0026lt; \u0026#34;Copy constructor invoked!\u0026#34; \u0026lt;\u0026lt; std::endl; } int main() { Cube c; Cube b; Cube myCube = c; // copy constructor will be called b = c //copy constructor will not be called return 0; }       Assign constructor\n  Is a public member function of the class.\n  Has the function name operator=\n  Has a return value of a reference of the class’ type.\n  Has exactly one argument\n  The argument must be const reference of the class’ type.\n1 2 3 4 5 6  Cube \u0026amp; Cube::operator= (const Cube \u0026amp; obj) { length_ = obj. length; std::cout \u0026lt;\u0026lt; \u0026#34;Assignment operator invoked!\u0026#34;\u0026lt;\u0026lt; std::endl; return *this; }         Destructor (~). An destructor should neverbe called directly. Instead, it is automatically called when the object’s memory is being reclaimed by the system:\n If the object is on the stack, when the function returns If the object is on the heap, when delete is used    ","description":null,"image":null,"permalink":"https://tsedao.github.io/post/c++/","subtitle":null,"tags":["C++"],"title":"C++ Learning Note"},{"content":"Docker Installation Install docker engine and Nvidia-docker on related platforms.\nProblem of seeing CUDA11 on docker while the host CUDA version in 11.\n One of the primary functions of nvidia-docker is to inject the all of NVIDIA driver libs from the host into the container so that the container will run properly with GPUs. One of these libraries is libcuda.so. This is one of the reasons you are seeing nvidia-smi report the driver version from your host.\n Different between nvidia docker image: base/runtime/devel\nExample Template Format of Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  FROMnvidia/cuda:10.2-cudnn7-runtime-ubuntu18.04ENV PATH=\u0026#34;/root/miniconda3/bin:${PATH}\u0026#34;ARG PATH=\u0026#34;/root/miniconda3/bin:${PATH}\u0026#34;RUN apt update \\  \u0026amp;\u0026amp; apt install -y htop python3-dev wgetRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\  \u0026amp;\u0026amp; mkdir root/.conda \\  \u0026amp;\u0026amp; sh Miniconda3-latest-Linux-x86_64.sh -b \\  \u0026amp;\u0026amp; rm -f Miniconda3-latest-Linux-x86_64.shRUN conda create -y -n env_name python=3.7COPY . home/RUN /bin/bash -c \u0026#34;cd home/ \\ \u0026amp;\u0026amp; source activate env_name \\ \u0026amp;\u0026amp; pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt\u0026#34;WORKDIRhome/CMD []  Build your customized image through Dockerfile 1  $ docker build -t nameOfyouImage .   1 2 3 4 5  $ docker run -it nameOfyouImage /bin/bash \\  -v absolute path of your localhost:absolute path of your remotehost \\  -p [porting] \\  --rm [run the container the remove it after exit it] --gpus   Docker run contains docker create and run two commands, it first creates a container based on one image, then it runs /bin/bash on the container. One can use exit to stop the container or can push Ctrl + P + Q to detach container\u0026rsquo;s terminal and keep the container running.\n1  $ docker ps   Checking the running container, option -a can be used to check the exited containers.\n1  $ docker start CONTAINERID   1 2  $ docker attach CONTAINERID $ docker exec -ti CONTAINERID # exec abd attach both require the container is running   Through docker ps  , we could use the ID number of the container to restart it and attach to it based on the specified commends.\nSaving and loading the Docker image Some of the servers or clusters are offline, so we may package the image we want and upload it to the remote server.\n1  $ docker commit # commit the change to the image   1  $ docker save myimage:latest | gzip \u0026gt; myimage_latest.tar.gz   1  $ docker image load -i myimage_latest.tar.gz   Source https://www.youtube.com/watch?v=0qG_0CPQhpg\n","description":null,"image":null,"permalink":"https://tsedao.github.io/post/docker/","subtitle":null,"tags":["Docker"],"title":"Docker Tutorial"},{"content":"THIS IS A SIMPLIFIED LECTURE REVIEW BASED ON CUHKSZ\u0026rsquo;S THEORY OF STATISTICS (Statistical Inference George Casella)\nCommon Family of Distributions Exponential Family $f(x|\\theta)=h(x)c(\\theta)\\exp(\\sum_{i}^{k}w_{i}(\\theta)t_{i}(x))$\n Binomial Distribution Poisson Distribution Exponential Distribution  Full exponential family (dimension k = number of parameters d) versus curved exponential family (dimension k \u0026gt; number of parameters d)\nWhy do we need exponential family?\n Simplify calculating exception and variance of different moments based on $t_{i}(x)$' We can use k statistics instead of a large number of original samples to make an inference about the parameter $\\theta$ equivalently, without losing any information. i.e., Good property of sufficiency and completeness.  Note: for some distribution, we need to recalculate its pdf or pmf using indicator function based on its support.\nLocation and Scale Family  Let $f(x)$ be any pdf. Then the family of pdfs $f(x-\\mu)$, indexed by the parameter $\\mu$ is called the location family.  If Z has a pdf $f(z)$, then $X=Z+\\mu$ has density $f(x-\\mu)$   Let $f(x)$ be any pdf. Then the family of pdfs $\\frac{1}{\\sigma}f(\\frac{x}{\\sigma})$ is called the scale family with standard pdf $f(x)$ How to prove the above statement? Event equivalence!  ${X = x } \\Leftrightarrow {Z = \\frac{X-\\mu}{\\sigma}}$ $P(X=x)=P(Z=\\frac{x-\\mu}{\\sigma})$ $f_{X}(x)dx=f(\\frac{x-\\mu}{\\sigma})dz$    Data Reduction Why reduction? Summarize the information in a sample by determining a few key features of the sample values\nHow to do reduction? The method that do not discard important information about unknown parameter $\\theta$ and methods that successfully discard information that is irrelevant as far as gaining knowledge about $\\theta$ is concerned.\nSufficiency Definition: $P(X=x | T(X)=T(x))$ is independent of $\\theta$. The conditional distribution of a sample $X$ given the value $T(X)$ does not depend on $\\theta$.\nIt turns out that outside the exponential family distribution, it is rare to have a sufficient statistic of smaller dimension of the sample.\nHow to find sufficiency statistic?\n $\\frac{p(x|\\theta)}{q(T(x|\\theta)}$ independent of $\\theta$ Factorization theorem $f(x|\\theta)=h(x)g(T(x)|\\theta)$ Using exponential family $\\sum_{i}^{n}t(X_{i})$  $T'(x) = r(T(x))$, if $T'$ is sufficient, then $T$ is also sufficient. The converse of the statement is false.\nMinimal Sufficiency Statistic Why we need Minimal Sufficiency Statistic (M.S.S)? Since we have already many sufficiency statistics and any one-to-one function of a sufficient statistic is a sufficient statistic. Among these sufficient statistic, we want to find one unique sufficient statistic that is a function of another sufficient statistic.\nDefinition: A sufficient statistic T is called minimal sufficient statistic if, for any other sufficient statistic T', T is a function of T'.\nHow to find M.S.S?\n $\\frac{f(x|\\theta)}{f(y|\\theta)}$ is independent of $\\theta$ if and only if $T(x) = T(y)$  Ancillary Definition: A statistic S(X) whose distribution does not depend on the parameter $\\theta$ is called ancillary statistic. (i.e. $P(S(X)| \\theta)$ independent of $\\theta$ )\nCompleteness Definition: For $g(T)$ satisfies $E_{\\theta}(g(T)) = 0$ for all $\\theta \\in \\Theta$ implies $P_{\\theta}(g(T)=0)=1$ for $\\theta \\in \\Theta$ . Then $T(X)$ is called complete statistic\nInterpretation:\n No ancillary statistic can be constructed based on complete statistic. Unique unbiased estimator, i.e., $E(T(x)) =\\tau(\\theta)$ if $T$ is a complete sufficient statistic  Disprove Completeness:\n Construct ancillary statistic $g(T)=S(T)-E(S(T))$, $P(g(T)) \\ne 0$ Just cancel First moment i.e. $E[h_{1}(T)]=E[h_{2}(T)]$, $g(t)=E[h_{1}(T)]-E[h_{2}(T)]$  Prove\n Direct calculate $E(g(T))=0$, or take the derivative related to $\\theta$ Using the nice property of Full exponential family i.e., d=k  Property of Complete Statistic:\n $T' = r(T)$, if T is complete, then T' is also complete. A complete statistic can be not sufficient.  Basu\u0026rsquo;s Theorem T(X) is complete and minimal sufficient statistic, then T(X) is independent of every ancillary statistic.\nBahadur\u0026rsquo;s Theorem C.S.S is in M.S.S.\nThere exists only two situation:\n If there exists a T(x) to be M.S.S., but not complete, then no sufficient statistic is complete. If there exists a T(x) such that T is C.S.S and M.S.S, then any M.S.S are also C.S.S.  Point Estimation Methods of Finding Point Estimators Method of Moment Drawback: sometimes the estimator will out of range and lack of numerical stability (i.e., a small change in the sample will lead to a huge change in the estimator.)\nMaximum Likelihood Estimation     Scalar Multidimension     Continuous Yes (Second order sufficient condition SOSC) Yes   Discrete Yes No (Discrete optimization)    Advantage: the range of MLE coincides the range of parameters.\nDrawback:\n  Difficult to find a global maximum\n  numerical sensitivity. That is how sensitive the estimator to the small change in the data.\n  Invariance property of MLE. i.e., MLE of $\\tau(\\theta)$ is $\\tau(\\hat{\\theta})$\nBayes Estimator Data: (x1,x2,\u0026hellip;xn) + Expert Knowledge $f(x|\\theta)$ or $\\pi(x)$\nCons: controversial because it inherently embraces a subjective notion of probability. It has no guarantee of long time performance\nPosterior distribution: $\\pi(\\theta|x) = f(x|\\theta)\\pi(\\theta)/ m(x)$\n$\\pi(x)$ Is said to conjugate to $f(x|\\theta)$, if $\\pi(\\theta|x)$ is in the same distribution family as $\\pi(x)$\nMethods of Evaluating Point Estimators Mean Square Error (MSE) MSE = $E_{\\theta}(W-\\theta)^{2}=Var_{\\theta}(W)+(Bias_{\\theta}(W))^{2}$\nMSE is not good for scale parameter (i.e. $\\sigma$ in normal distribution), since it MSE penalizes equally for overestimation and underestimation. And the scale case, 0 is a natural lower bound, so the estimation problem is not symmetric.\nBest Unbiased Estimator (Minimize variance and control bias) Motivation: many times we can a estimator that is uniformly better than the other estimator. (i.e., MSE($\\hat{\\sigma}^{2}$)\u0026lt;MSE($S^{2}$) for any $\\sigma^{2} \u0026gt; 0$). If we can find an unbiased estimator with uniformly smallest variance, a best unbiased estimator, then our task is done.\n  UMVUE (uniform minimum variance unbiased estimator)\n  Cramer-Rao Inequality Cramer-Rao Lower Bound valid or not?\nWe can specify a lower bound, say $B(\\theta)$, on the variance of any unbiased estimator of $\\tau(\\theta)$. If we can attain such lower bound, we can say we have found a best unbiased estimator.\n  Attainment\nSince sometimes there is no guarantee that the bound is sharp, we need to find out wether we can attain the Lower bound or not.\n  Use of sufficiency and unbiasedness\nRao-Blackwell gives decrease property conditioned on sufficiency\nCondition a unbiased statistic based on a sufficient statistic $T$, i.e., $\\phi(T)=E(W|T)$， then $E_{\\theta}\\phi(T)=\\tau(\\theta)$ and $Var_{\\theta}(\\phi(T)) \\leq Var_{\\theta}(W)$ for all $\\theta$.\nUniqueness of best estimator\n DISPROVE BEST ESTIMATOR (IF IT IS NOT ATTAINED BY CRLB): If $E_{\\theta}W = \\tau(\\theta)$, W is the best unbiased estimator of $\\tau(\\theta)$ if and only if $W$ is uncorrelated with all unbiased estimators of 0.\n  If T is complete and sufficient statistic, then $E(W|T)$ is the best unbiased estimator of $\\tau(\\theta)$, since $\\phi(T)$ is uncorrelated with any unbiased estimator of 0. So that it gives the best estimator\n   Minimize Risk Function Hypothesis test Definition: A hypothesis is a a statement about a population parameter. A hypothesis test is a rule that specifies 1. For which sample values $H_{0}$ is rejected and for which sample value $H_{0}$ is accepted to be true.\nFind Hypothesis test Frequentist view point: Compare the likelihood of H0 and H1. Likelihood Ratio Test (LRT), Union-Intersection, Intersection-Union gives rejection region $X\\in R = {\\lambda(x)\u0026lt;c}$\nBayesian view point: Compare the probability of H0 and H1\nEvaluate Hypothesis test Generally, we have to control Type 1 error (incorrectly reject H0) and Type 2 error (incorrectly accept H0)\nPower function  Power function $P_{\\theta}(X \\in R) = \\beta(\\theta)$ that quantifies both Type 1 and Type 2 error)  Specifically, we want to control Type 1 error while minimizing Type 2 error. i.e Size alpha test \u0026lsquo;'=\u0026rsquo;' and level alpha test \u0026ldquo;$\\le$\u0026rdquo; restrict c in some degree $sup_{\\theta \\in \\theta_{0}} P(X\\in R) = \\alpha$, control the type 1 error    Most Powerful Test   Size alpha test and level alpha test may not be unique. We need to minimize Type 2 error and control Type 1 error simultaneously\n  UMP (uniformly most powerful) test further impose hard restriction (Type 2 error) test from level alpha test.\n Neyman-Pearson for simple hypothesis (UMP). Karlin Rubin for one-side hypothesis (UMP) Non-existence for two-side hypothesis Sometimes UMP does not exist, what we should do? (Two-side hypothesis)    P-value Definition: P-value is the probability of getting a value of test statistic that is at least as extreme as the one representing the sample data. $P(W(X)\u0026gt;W(x)|H_{0} \\ is \\ true)$\nP value reports the test results on a continuous case, rather than a dichotomous case \u0026lsquo;accept\u0026rsquo; and \u0026lsquo;reject\u0026rsquo;\nP-value cannot quantify Type 2 error and the definition of extremeness is vague.\nWe can construct $P(X) = sup_{\\theta \\in \\Theta_{0}}P_{\\theta}(W(X) \\ge W(x))$ or $P(W(X)\\ge W(x) | S(X)=S(x))$ conditioned on sufficient statistic.\nInterval Estimation Every confidence set corresponds to a hypothesis test and vice versa.\nMethods of finding Interval Estimation   Inverted LRT Step 1: find LRT accept region, Step 2: turn it to explicit form, Step 3: show that exist for all $\\theta$\n  Pivot Quantiles\n  Methods of Evaluating Interval Estimation   Minimize the size (length) of interval and control the coverage probability. (Solved by Lagrange multipliers with equality constraints.)\n  Decision Theory. $R(\\theta,c)= E_{\\theta}[L(\\theta, c)]=bE_{\\theta}[Length(C(X))]-P_{\\theta}(\\theta \\in C(X))$\n  ","description":null,"image":null,"permalink":"https://tsedao.github.io/post/theoryofstat/","subtitle":null,"tags":null,"title":"Theory Of Statistics"},{"content":"Search  Go \u0026nbsp;      ","description":null,"image":null,"permalink":"https://tsedao.github.io/search/","subtitle":null,"tags":null,"title":""}]